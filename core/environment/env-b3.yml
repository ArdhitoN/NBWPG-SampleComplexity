# Assuming infinite-horizon, finite MDP, state set and action set is fixed and finite.
# Therefore, the number of stationary deterministic policies is |A|^|S|
# Means the number of possible actions on each states will be the same
transitions:
  s0:
    a00: { "next_state": "s1", "reward": 5, "prob": 1.0 }
    a01: { "next_state": "s1", "reward": 10, "prob": 1.0 }
  s1:
    a10: { "next_state": "s2", "reward": 8, "prob": 1.0 }
    a11: { "next_state": "s2", "reward": 1, "prob": 1.0 }
  s2:
    a20:
      - { "next_state": "s3", "reward": 0, "prob": 0.8 }
      - { "next_state": "s2", "reward": 2, "prob": 0.2 }
    a21: 
      - { "next_state": "s2", "reward": 2, "prob": 0.8 }
      - { "next_state": "s3", "reward": 0, "prob": 0.2 }
  s3:
    a30: 
      - { "next_state": "s4", "reward": 0, "prob": 0.8 }
      - { "next_state": "s2", "reward": 2, "prob": 0.2 }

    a31: 
      - { "next_state": "s2", "reward": 2, "prob": 0.8 }
      - { "next_state": "s4", "reward": 0, "prob": 0.2 }

  s4:
    a40: 
      - { "next_state": "s5", "reward": 0, "prob": 0.8 }
      - { "next_state": "s2", "reward": 2, "prob": 0.2 }

    a41: 
      - { "next_state": "s2", "reward": 2, "prob": 0.8 }
      - { "next_state": "s5", "reward": 0, "prob": 0.2 }

  s5:
    a50: 
      - { "next_state": "s6", "reward": 0, "prob": 0.8 }
      - { "next_state": "s2", "reward": 2, "prob": 0.2 }

    a51: 
      - { "next_state": "s2", "reward": 2, "prob": 0.8 }
      - { "next_state": "s6", "reward": 0, "prob": 0.2 }
  s6:
    a60: 
      - { "next_state": "s6", "reward": 10, "prob": 0.8 }
      - { "next_state": "s2", "reward": 2, "prob": 0.2 }

    a61: 
      - { "next_state": "s2", "reward": 2, "prob": 0.8 }
      - { "next_state": "s6", "reward": 10, "prob": 0.2 }

transitions_converted:
  0:
    0:
      1: { "prob": 1.0, "reward": 5 }
    1:
      1: { "prob": 1.0, "reward": 10 }
  1:
    0:
      2: { "prob": 1.0, "reward": 8 }
    1:
      2: { "prob": 1.0, "reward": 1 }
  2:
    0:
      3: { "prob": 0.8, "reward": 0 }
      2: { "prob": 0.2, "reward": 2 }
    1:
      2: { "prob": 0.8, "reward": 2 }
      3: { "prob": 0.2, "reward": 0 }
  3:
    0:
      4: { "prob": 0.8, "reward": 0 }
      2: { "prob": 0.2, "reward": 2 }
    1:
      2: { "prob": 0.8, "reward": 2 }
      4: { "prob": 0.2, "reward": 0 }
  4:
    0:
      5: { "prob": 0.8, "reward": 0 }
      2: { "prob": 0.2, "reward": 2 }
    1:
      2: { "prob": 0.8, "reward": 2 }
      5: { "prob": 0.2, "reward": 0 }
  5:
    0:
      6: { "prob": 0.8, "reward": 0 }
      2: { "prob": 0.2, "reward": 2 }
    1:
      2: { "prob": 0.8, "reward": 2 }
      6: { "prob": 0.2, "reward": 0 }
  6:
    0:
      6: { "prob": 0.8, "reward": 10 }
      2: { "prob": 0.2, "reward": 2 }
    1:
      2: { "prob": 0.8, "reward": 2 }
      6: { "prob": 0.2, "reward": 10 }

initial_state: "s0"
initial_state_dist: { 0: 1.0 }
