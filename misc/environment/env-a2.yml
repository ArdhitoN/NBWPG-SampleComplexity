# Assuming infinite-horizon, finite MDP, state set and action set is fixed and finite.
# Therefore, the number of stationary deterministic policies is |A|^|S|
# Means the number of possible actions on each states will be the same
transitions:
  s0:
    a00: { "next_state": "s1", "reward": 1, "prob": 1.0 }
    a01: { "next_state": "s2", "reward": 0, "prob": 1.0 }
  s1:
    a10: { "next_state": "s3", "reward": 1, "prob": 1.0 }
    a11: { "next_state": "s3", "reward": 1, "prob": 1.0 }
  s2:
    a20: { "next_state": "s2", "reward": 3, "prob": 1.0 }
    a21: { "next_state": "s1", "reward": 3, "prob": 1.0 }
  s3:
    a30: { "next_state": "s4", "reward": 3, "prob": 1.0 }
    a31: { "next_state": "s4", "reward": 0, "prob": 1.0 }
  s4:
    a40: { "next_state": "s4", "reward": 0, "prob": 1.0 }
    a41: { "next_state": "s4", "reward": 0, "prob": 1.0 }
  
initial_state: "s0"

transitions_converted:
  # Index at each level consecutively : current state, action, next state
  0:
    0:
      1: { "prob": 1.0, "reward": 1 }
      
    1:
      2: { "prob": 1.0, "reward": 0 }

  1:
    0:
      3: { "prob": 1.0, "reward": 1 }
    1:
      3: { "prob": 1.0, "reward": 1 }
  
  2:
    0:
      3: { "prob": 1.0, "reward": 3 }
    1:
      3: { "prob": 1.0, "reward": 3 }

  3:
    0:
      4: { "prob": 1.0, "reward": 3 }
    1:
      4: { "prob": 1.0, "reward": 0 }

  4:
    0:
      4: { "prob": 1.0, "reward": 0 }
    1:
      4: { "prob": 1.0, "reward": 0 }
      

initial_state: "s0"
initial_state_dist: { 0: 1.0 }
