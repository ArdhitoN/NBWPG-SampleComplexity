# Assuming infinite-horizon, finite MDP, state set and action set is fixed and finite.
# Therefore, the number of stationary deterministic policies is |A|^|S|
# Means the number of possible actions on each states will be the same


# Gridworld
# s2 s3 
# s0 s1
# s0 --> initial state
# s3 --> goal state

transitions:
  s0:
    a00: # East
      - { "next_state": "s1", "reward": -1, "prob": 0.9 }
      - { "next_state": "s2", "reward": -1, "prob": 0.1 }

    a01: # North 
      - { "next_state": "s2", "reward": -1, "prob": 0.9 }
      - { "next_state": "s1", "reward": -1, "prob": 0.1 }

  s1:
    a10: # West 
        - { "next_state": "s0", "reward": -1, "prob": 0.9 }
        - { "next_state": "s3", "reward": -1, "prob": 0.1 }

    a11: # North
      - { "next_state": "s3", "reward": -1, "prob": 0.9 }
      - { "next_state": "s0", "reward": -1, "prob": 0.1 }

  s2:
    a20: # East 
      - { "next_state": "s3", "reward": -1, "prob": 0.9 }
      - { "next_state": "s0", "reward": -1, "prob": 0.1 }
    a21: # South
      - { "next_state": "s0", "reward": -1, "prob": 0.9 }
      - { "next_state": "s3", "reward": -1, "prob": 0.1 }

  s3:
    a30: # Self transition 
      - { "next_state": "s3", "reward": 0, "prob": 1.0 }
    a31: # Self transition
      - { "next_state": "s3", "reward": 0, "prob": 1.0 }
  
transitions_converted:
  # Index at each level consecutively : current state, action, next state
  0:
    0:
      1: { "prob": 0.9, "reward": -1 }
      2: { "prob": 0.1, "reward": -1 }

    1:
      2: { "prob": 0.9, "reward": -1 }
      1: { "prob": 0.1, "reward": -1 }

  1:
    0:
      0: { "prob": 0.9, "reward": -1 }
      3: { "prob": 0.1, "reward": -1 }
    1:
      3: { "prob": 0.9, "reward": -1 }
      0: { "prob": 0.1, "reward": -1 }

  2:
    0:
      3: { "prob": 0.9, "reward": -1 }
      0: { "prob": 0.1, "reward": -1 }
    1:
      0: { "prob": 0.9, "reward": -1 }
      3: { "prob": 0.1, "reward": -1 }

  3:
    0:
      3: { "prob": 1.0, "reward": 0 }
    1:
      3: { "prob": 1.0, "reward": 0 }

initial_state: "s0"
initial_state_dist: { 0: 1.0 }
